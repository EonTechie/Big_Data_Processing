{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8/QmV1dzw/L25XpPamcMk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EonTechie/Big_Data_Processing_Spark_Projects/blob/main/spark-rdd-tasks/CurrencyDailyIncreaseDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filiz-Yıldız-Part1-Question5\n",
        "\n",
        "\"\"\"\n",
        "Dataset:\n",
        "DollarDataset.txt\n",
        "\n",
        "Goal:\n",
        "Find the top 5 days with the highest percentage increase in dollar value.\n",
        "\n",
        "My Approach:\n",
        "I solved this question in two different ways:\n",
        "\n",
        "1. Method (as requested) : Using zipWithIndex (RDD-based):\n",
        "I added an index to each row in the RDD using zipWithIndex().\n",
        "Then, I joined each current value with its previous one based on the index and calculated the percentage change.\n",
        "This method allowed me to simulate a \"lag\" operation using only pure RDD transformations.\n",
        "\n",
        "2. Extra Method: Using lag() window function (DataFrame-based):\n",
        "I also solved the same problem using Spark DataFrames by applying the lag() window function.\n",
        "This gave me direct access to the previous row, and I used it to compute percentage increases more easily.\n",
        "\n",
        "Result:\n",
        "After calculating percentage changes in both approaches, I sorted the results in descending order\n",
        "and selected the top 5 days with the highest percentage increases.\n",
        "\n",
        "This question helped me practice both RDD and DataFrame techniques, and compare their flexibility for time-based analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Connect colab to my drive account to fetch the dataset stored there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Print files to see the namesof all (optional)\n",
        "import os\n",
        "folder_path = \"/content/drive/My Drive/datasets\"\n",
        "files = os.listdir(folder_path)\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRvRW5QYMxsh",
        "outputId": "5f3a646c-34df-44c4-d492-4a46be6d0288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['2.txt', 'Capitals.txt', 'EartquakeData-07032025.txt', 'DollarDataset.txt', 'couples.txt', 'join-actors.txt', 'points-null-values.txt', 'numbers-test.txt', 'join-series.txt', 'points.txt', 'names.txt', 'Lottery.txt', 'JamesJoyce-Ulyses.txt', 'world.txt', 'points-places.txt', 'Iris.csv', 'ml-latest-small']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXdeggNf8DL6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession  # Import SparkSession to create and manage Spark applications\n",
        "\n",
        "# Create or get a Spark session with the name\n",
        "# This session is the entry point to use DataFrame and SQL functionalities in PySpark\n",
        "spark = SparkSession.builder.appName(\"Part1-Question5-DollarDataset\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I solved the problem using 'zipWithIndex' first"
      ],
      "metadata": {
        "id": "vY8OurrdPuBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file as an RDD (Resilient Distributed Dataset) using Spark\n",
        "# The textFile() method reads the text file and loads it as an RDD, where each line is an element in the RDD\n",
        "rdd = spark.sparkContext.textFile(\"/content/drive/My Drive/datasets/DollarDataset.txt\")\n",
        "\n",
        "# Take the first 5 lines of the RDD to inspect the data\n",
        "# The 'take(5)' function retrieves the first 5 elements (lines) from the RDD\n",
        "rdd.take(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSnwFBCgQVSE",
        "outputId": "6640862b-957e-40dc-9fda-205c77caea0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1\\t02-01-1950\\t2,80',\n",
              " '2\\t03-01-1950\\t2,80',\n",
              " '3\\t04-01-1950\\t2,80',\n",
              " '4\\t05-01-1950\\t2,80',\n",
              " '5\\t06-01-1950\\t2,80']"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split each line of the RDD by the tab character (\"\\t\")\n",
        "# The map() function applies the lambda function to each line in the RDD, splitting each line into a list based on tab separation\n",
        "rdd_split = rdd.map(lambda line: line.split(\"\\t\"))\n",
        "\n",
        "# Take the first 5 elements (lines) of the split RDD to check the result\n",
        "# This will show the first 5 lines, where each line is now split into a list of values\n",
        "print(rdd_split.take(5))\n",
        "rdd_split.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltxupp8gQVU3",
        "outputId": "999d49ff-0ac7-4034-d4a9-226fd969470c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['1', '02-01-1950', '2,80'], ['2', '03-01-1950', '2,80'], ['3', '04-01-1950', '2,80'], ['4', '05-01-1950', '2,80'], ['5', '06-01-1950', '2,80']]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17776"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if a string can be converted to a valid float\n",
        "# It replaces commas with dots (for decimal consistency) and attempts to convert the value to float\n",
        "def is_valid_float(val):\n",
        "    try:\n",
        "        float(val.replace(\",\", \".\"))  # Replace comma with dot and try to convert to float\n",
        "        return True\n",
        "    except:\n",
        "        return False  # If conversion fails, return False\n",
        "\n",
        "# Filter the RDD to keep only the rows with 3 columns, where the third column is not empty\n",
        "# Additionally, the third column must be a valid float value\n",
        "# This ensures only valid rows (with 3 columns and a valid float value in the third column) are kept\n",
        "rdd_clean = rdd_split.filter(lambda x: len(x) == 3 and x[2] != '' and is_valid_float(x[2]))\n",
        "\n",
        "# Show the first 5 rows of the cleaned RDD to verify the result\n",
        "print(rdd_clean.take(5))\n",
        "rdd_clean.count()\n",
        "\n",
        "# Check the types of the individual columns in the first row\n",
        "print(\"Third column type in the current rdd : \", type(rdd_clean.take(1)[0][2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7J5N5uzQVXj",
        "outputId": "3c3d1be8-4bee-4eda-c240-173a40d19d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['1', '02-01-1950', '2,80'], ['2', '03-01-1950', '2,80'], ['3', '04-01-1950', '2,80'], ['4', '05-01-1950', '2,80'], ['5', '06-01-1950', '2,80']]\n",
            "Third column type in the current rdd :  <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the third column to float and create a new RDD with 3 columns\n",
        "rdd_float = rdd_clean.map(lambda x: (x[0], x[1], float(x[2].replace(\",\", \".\"))))\n",
        "\n",
        "# Check the types of the individual columns in the first row\n",
        "print(\"Third column type in the current rdd : \", type(rdd_float.take(1)[0][2]))\n",
        "\n",
        "# Take the first row of rdd_float to check the types of each element\n",
        "rdd_float.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxzJbAhIQVaz",
        "outputId": "ba6193be-2d71-483a-ba64-ed9edcc8806c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Third column type in the current rdd :  <class 'float'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '02-01-1950', 2.8),\n",
              " ('2', '03-01-1950', 2.8),\n",
              " ('3', '04-01-1950', 2.8),\n",
              " ('4', '05-01-1950', 2.8),\n",
              " ('5', '06-01-1950', 2.8)]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I did dthis extra for checking and analytical purposes\n",
        "# Extract the first column (index) from the rdd\n",
        "indexes = rdd_float.map(lambda x: int(x[0]))  # Assuming the first column is the index and is a string that needs to be converted to an integer\n",
        "\n",
        "# Check if the indexes are sequential and find the first non-sequential index\n",
        "sequential_errors = indexes.zipWithIndex().filter(lambda x: x[0] != x[1] + 1).collect()\n",
        "\n",
        "# Output the result\n",
        "if sequential_errors:\n",
        "    print(\"First non-sequential index:\", sequential_errors[0])\n",
        "else:\n",
        "    print(\"The indexes are sequential.\")\n",
        "\n",
        "# By doing this so I acknowledged that I need to use indexes which I will set with zipWithIndex()\n",
        "# (Because exist ones even were not in a sequential form, and also there are some data need to be cleaned like some values are lack missing in the file so that indexes cnannot be used)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vyaHcnmSG7M",
        "outputId": "121ccdc5-0f14-45ee-c187-2960db19cb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First non-sequential index: (101, 99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply zipWithIndex() to the rdd_float to add an index to each row\n",
        "\n",
        "rdd_indexed = rdd_float.zipWithIndex() # ((index, date, value), index)\n",
        "\n",
        "# Show the first 5 rows of the indexed RDD to verify the result\n",
        "rdd_indexed.take(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr0DjFdqQVdB",
        "outputId": "b9bf1859-0553-4e74-d018-e1e6cedd2d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('1', '02-01-1950', 2.8), 0),\n",
              " (('2', '03-01-1950', 2.8), 1),\n",
              " (('3', '04-01-1950', 2.8), 2),\n",
              " (('4', '05-01-1950', 2.8), 3),\n",
              " (('5', '06-01-1950', 2.8), 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I changed the places of index to the left side to use groupByKey afterwards, also not taking incorrect ordered indexing of columns coming from file\n",
        "current = rdd_indexed.map(lambda x: (int(x[1]), ( x[0][1], x[0][2])))   # (index, date, value)\n",
        "current.take(5)\n",
        "\n",
        "# The result is a tuple where the first element is the original row (x[0]) and the second element is the index (x[1])\n",
        "# We then map each row to a new tuple (index, (date, value)) where:\n",
        "# - x[0] is the index\n",
        "# - x[1][0] is the date (first column)\n",
        "# - x[1][1] is the value (second column, already a float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLvTSECTMe8t",
        "outputId": "23486bd5-5569-4f17-9b71-f16fb5a1be6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, ('02-01-1950', 2.8)),\n",
              " (1, ('03-01-1950', 2.8)),\n",
              " (2, ('04-01-1950', 2.8)),\n",
              " (3, ('05-01-1950', 2.8)),\n",
              " (4, ('06-01-1950', 2.8))]"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decrease the key by 1 to align it with the next record (for joining later)\n",
        "# (x[0] - 1) → shifts the key back by one\n",
        "# (x[1][0], x[1][1]) → keeps the date and value unchanged\n",
        "prev = current.map(lambda x: (x[0] - 1, (x[1][0], x[1][1])))\n",
        "\n",
        "# Show the first 5 results\n",
        "prev.take(5)\n"
      ],
      "metadata": {
        "id": "jmwAjpYwrfTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ac55bf-19a7-4ae6-c6ce-b6afb33dbd7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-1, ('02-01-1950', 2.8)),\n",
              " (0, ('03-01-1950', 2.8)),\n",
              " (1, ('04-01-1950', 2.8)),\n",
              " (2, ('05-01-1950', 2.8)),\n",
              " (3, ('06-01-1950', 2.8))]"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine current and previous RDDs using union\n",
        "# Then sort the combined RDD by the key (x[0])\n",
        "rdd = current.union(prev).sortBy(lambda x: x[0])\n",
        "\n",
        "# Show the first 5 results\n",
        "rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYyUBRd5XtZO",
        "outputId": "65acd091-992a-4b04-ac59-8d8988b4290a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-1, ('02-01-1950', 2.8)),\n",
              " (0, ('02-01-1950', 2.8)),\n",
              " (0, ('03-01-1950', 2.8)),\n",
              " (1, ('03-01-1950', 2.8)),\n",
              " (1, ('04-01-1950', 2.8))]"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group values by key as indexes of previos day and next day will be in the same tuple\n",
        "# Convert the grouped values (which are iterable) into a tuple\n",
        "# Then sort the result by the key\n",
        "resultRDD = rdd.groupByKey().map(lambda x: (x[0], tuple(x[1]))).sortBy(lambda x: x[0])\n",
        "\n",
        "# Show the first 5 results\n",
        "resultRDD.take(5)"
      ],
      "metadata": {
        "id": "6gVohmm9r18Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7847bdc9-73c0-40aa-affd-4321d6ef35ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-1, (('02-01-1950', 2.8),)),\n",
              " (0, (('02-01-1950', 2.8), ('03-01-1950', 2.8))),\n",
              " (1, (('03-01-1950', 2.8), ('04-01-1950', 2.8))),\n",
              " (2, (('04-01-1950', 2.8), ('05-01-1950', 2.8))),\n",
              " (3, (('05-01-1950', 2.8), ('06-01-1950', 2.8)))]"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the records where the value has exactly 2 items (paired entries),\n",
        "# because the first and last records have only 1 item,\n",
        "# which otherwise cause an index out of range error later\n",
        "filteredRDD = resultRDD.filter(lambda x: len(x[1]) == 2)\n",
        "\n",
        "# Show the first 5 results\n",
        "filteredRDD.take(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnh0PuBUgLY7",
        "outputId": "03f973ab-2d74-4fe4-d298-2d1d6a5e3466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, (('02-01-1950', 2.8), ('03-01-1950', 2.8))),\n",
              " (1, (('03-01-1950', 2.8), ('04-01-1950', 2.8))),\n",
              " (2, (('04-01-1950', 2.8), ('05-01-1950', 2.8))),\n",
              " (3, (('05-01-1950', 2.8), ('06-01-1950', 2.8))),\n",
              " (4, (('06-01-1950', 2.8), ('09-01-1950', 2.8)))]"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new RDD with:\n",
        "# - a string showing the date range: \"from <oldDate> to <newDate>\"\n",
        "# - percentage change: ((new - old) / old) * 100\n",
        "#   If the old value is 0, return None to safely avoid division by zero\n",
        "rdd_pct = filteredRDD.map(lambda x: (\n",
        "    \"from \" + x[1][0][0] + \" to \" + x[1][1][0],\n",
        "    ((x[1][1][1] - x[1][0][1]) / x[1][0][1] * 100) if x[1][0][1] != 0 else None\n",
        "))\n",
        "\n",
        "# Show the first 5 results\n",
        "rdd_pct.take(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WarBiSfddfl",
        "outputId": "8f36e331-6883-4a1d-a5ae-b51b104c7b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('from 02-01-1950 to 03-01-1950', 0.0),\n",
              " ('from 03-01-1950 to 04-01-1950', 0.0),\n",
              " ('from 04-01-1950 to 05-01-1950', 0.0),\n",
              " ('from 05-01-1950 to 06-01-1950', 0.0),\n",
              " ('from 06-01-1950 to 09-01-1950', 0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the RDD by the percentage change in descending order\n",
        "# x[1] is the percentage value, so we use -x[1] for descending order\n",
        "# If the percentage is None (to avoid division by zero), treat it as very small (negative infinity)\n",
        "sorted_rdd = rdd_pct.sortBy(lambda x: -x[1])\n",
        "\n",
        "# Show the top 5 results\n",
        "sorted_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk6VlpQIr6V-",
        "outputId": "1a165ac9-048a-47b8-98b6-deb63cecf5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('from 19-08-1960 to 22-08-1960', 221.42857142857144),\n",
              " ('from 24-01-1980 to 25-01-1980', 100.0),\n",
              " ('from 07-08-1970 to 10-08-1970', 64.99999999999999),\n",
              " ('from 11-06-1979 to 12-06-1979', 32.075471698113205),\n",
              " ('from 28-02-1978 to 01-03-1978', 29.87012987012987)]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print header\n",
        "print(\"Date Range                    |  Percentage Change\")\n",
        "print(\"----------------------------------------------\")\n",
        "\n",
        "# Print the top 5 rows from the sorted RDD\n",
        "for row in sorted_rdd.take(5):\n",
        "    print(f\"{row[0]} | {row[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkIXiW-R06ou",
        "outputId": "5709a5ed-c5b6-4c24-d278-19cee67ad289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date Range                    |  Percentage Change\n",
            "----------------------------------------------\n",
            "from 19-08-1960 to 22-08-1960 | 221.42857142857144\n",
            "from 24-01-1980 to 25-01-1980 | 100.0\n",
            "from 07-08-1970 to 10-08-1970 | 64.99999999999999\n",
            "from 11-06-1979 to 12-06-1979 | 32.075471698113205\n",
            "from 28-02-1978 to 01-03-1978 | 29.87012987012987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the sorted RDD to a DataFrame with proper column names\n",
        "df = sorted_rdd.toDF([\"DateRange\", \"PercentageChange\"])\n",
        "\n",
        "# Display the first 5 rows without truncating the content\n",
        "df.show(5, False)\n",
        "\n",
        "# In the dataset, some dates are missing,\n",
        "# so the result may show jumps between non-consecutive dates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqK8RyLD07Jt",
        "outputId": "feca36f6-031b-4e40-e41d-757a99f44518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+------------------+\n",
            "|DateRange                    |PercentageChange  |\n",
            "+-----------------------------+------------------+\n",
            "|from 19-08-1960 to 22-08-1960|221.42857142857144|\n",
            "|from 24-01-1980 to 25-01-1980|100.0             |\n",
            "|from 07-08-1970 to 10-08-1970|64.99999999999999 |\n",
            "|from 11-06-1979 to 12-06-1979|32.075471698113205|\n",
            "|from 28-02-1978 to 01-03-1978|29.87012987012987 |\n",
            "+-----------------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################################################################\n",
        "################################################################################################################################################################################\n",
        "################################################################################################################################################################################\n",
        "#################################################################################### EXTRA SECOND WAY SOLUTION #################################################################\n",
        "################################################################################################################################################################################\n",
        "################################################################################################################################################################################\n",
        "################################################################################################################################################################################"
      ],
      "metadata": {
        "id": "G957eOkoZnND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then I wanted to try another method to solve it using 'window' and 'lag'"
      ],
      "metadata": {
        "id": "fQs-RsxB1Wgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lag, regexp_replace, round  # Import Spark SQL functions for column operations and transformations\n",
        "from pyspark.sql.window import Window  # Import windowing functionality to apply functions across rows (like lag)\n",
        "\n",
        "# Start a Spark session named \"Question5\" for this analysis\n",
        "spark = SparkSession.builder.appName(\"Question5\").getOrCreate()\n",
        "\n",
        "# Read the tab-separated dataset without headers and infer column types\n",
        "df = (\n",
        "    spark.read\n",
        "    .option(\"inferSchema\", \"true\")   # Automatically detect column data types (e.g., double, int)\n",
        "    .option(\"delimiter\", \"\\t\")       # Specify tab as the delimiter between columns\n",
        "    .option(\"header\", \"false\")       # The dataset does not contain a header row\n",
        "    .csv(\"/content/drive/My Drive/datasets/DollarDataset.txt\")\n",
        "    )\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure and contents\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu5hYtveMBZ2",
        "outputId": "7cc0ffb2-95d7-4bd7-9d0b-200a97731ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----+\n",
            "|_c0|       _c1| _c2|\n",
            "+---+----------+----+\n",
            "|  1|02-01-1950|2,80|\n",
            "|  2|03-01-1950|2,80|\n",
            "|  3|04-01-1950|2,80|\n",
            "|  4|05-01-1950|2,80|\n",
            "|  5|06-01-1950|2,80|\n",
            "|  6|09-01-1950|2,80|\n",
            "|  7|10-01-1950|2,80|\n",
            "|  8|11-01-1950|2,80|\n",
            "|  9|12-01-1950|2,80|\n",
            "| 10|13-01-1950|2,80|\n",
            "| 11|16-01-1950|2,80|\n",
            "| 12|17-01-1950|2,80|\n",
            "| 13|18-01-1950|2,80|\n",
            "| 14|19-01-1950|2,80|\n",
            "| 15|20-01-1950|2,80|\n",
            "| 16|23-01-1950|2,80|\n",
            "| 17|24-01-1950|2,80|\n",
            "| 18|25-01-1950|2,80|\n",
            "| 19|26-01-1950|2,80|\n",
            "| 20|27-01-1950|2,80|\n",
            "+---+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the default column names (_c0, _c1, _c2) to more meaningful ones\n",
        "df1 = (\n",
        "    df.withColumnRenamed(\"_c0\", \"Index\")     # Rename _c0 to Index (could be row number or unique ID)\n",
        "      .withColumnRenamed(\"_c1\", \"Date\")      # Rename _c1 to Date (represents the date of the value)\n",
        "      .withColumnRenamed(\"_c2\", \"Value\")     # Rename _c2 to Value (the dollar value for that date)\n",
        ")\n",
        "\n",
        "df1.show(20, False)  # Display the renamed DataFrame to verify column names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k192SfTHMxLe",
        "outputId": "b050088e-7cc1-43d5-8d22-f90da247d0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|Index|Date      |Value|\n",
            "+-----+----------+-----+\n",
            "|1    |02-01-1950|2,80 |\n",
            "|2    |03-01-1950|2,80 |\n",
            "|3    |04-01-1950|2,80 |\n",
            "|4    |05-01-1950|2,80 |\n",
            "|5    |06-01-1950|2,80 |\n",
            "|6    |09-01-1950|2,80 |\n",
            "|7    |10-01-1950|2,80 |\n",
            "|8    |11-01-1950|2,80 |\n",
            "|9    |12-01-1950|2,80 |\n",
            "|10   |13-01-1950|2,80 |\n",
            "|11   |16-01-1950|2,80 |\n",
            "|12   |17-01-1950|2,80 |\n",
            "|13   |18-01-1950|2,80 |\n",
            "|14   |19-01-1950|2,80 |\n",
            "|15   |20-01-1950|2,80 |\n",
            "|16   |23-01-1950|2,80 |\n",
            "|17   |24-01-1950|2,80 |\n",
            "|18   |25-01-1950|2,80 |\n",
            "|19   |26-01-1950|2,80 |\n",
            "|20   |27-01-1950|2,80 |\n",
            "+-----+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = (\n",
        "        df1.withColumn(\"Value\",               # Target the 'Value' column\n",
        "        regexp_replace(\"Value\", \",\", \".\")     # Replace commas with dots for decimal consistency\n",
        "        .cast(\"float\")                        # Convert the cleaned string to float\n",
        "    )\n",
        ")\n",
        "\n",
        "df2.show()  # Show the DataFrame to verify numeric conversion\n",
        "df2.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCWEtl76Nxn8",
        "outputId": "0eac88b7-6134-4145-9f76-07efd7244af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|Index|      Date|Value|\n",
            "+-----+----------+-----+\n",
            "|    1|02-01-1950|  2.8|\n",
            "|    2|03-01-1950|  2.8|\n",
            "|    3|04-01-1950|  2.8|\n",
            "|    4|05-01-1950|  2.8|\n",
            "|    5|06-01-1950|  2.8|\n",
            "|    6|09-01-1950|  2.8|\n",
            "|    7|10-01-1950|  2.8|\n",
            "|    8|11-01-1950|  2.8|\n",
            "|    9|12-01-1950|  2.8|\n",
            "|   10|13-01-1950|  2.8|\n",
            "|   11|16-01-1950|  2.8|\n",
            "|   12|17-01-1950|  2.8|\n",
            "|   13|18-01-1950|  2.8|\n",
            "|   14|19-01-1950|  2.8|\n",
            "|   15|20-01-1950|  2.8|\n",
            "|   16|23-01-1950|  2.8|\n",
            "|   17|24-01-1950|  2.8|\n",
            "|   18|25-01-1950|  2.8|\n",
            "|   19|26-01-1950|  2.8|\n",
            "|   20|27-01-1950|  2.8|\n",
            "+-----+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17776"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.na.drop(subset=[\"Value\"])  # Drop rows where the 'Value' column is null (missing values)\n",
        "df3.show()  # Display the cleaned DataFrame\n",
        "df3.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU2j_iGSN0MH",
        "outputId": "8a819611-a56a-482a-b92c-6de11e9ea521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|Index|      Date|Value|\n",
            "+-----+----------+-----+\n",
            "|    1|02-01-1950|  2.8|\n",
            "|    2|03-01-1950|  2.8|\n",
            "|    3|04-01-1950|  2.8|\n",
            "|    4|05-01-1950|  2.8|\n",
            "|    5|06-01-1950|  2.8|\n",
            "|    6|09-01-1950|  2.8|\n",
            "|    7|10-01-1950|  2.8|\n",
            "|    8|11-01-1950|  2.8|\n",
            "|    9|12-01-1950|  2.8|\n",
            "|   10|13-01-1950|  2.8|\n",
            "|   11|16-01-1950|  2.8|\n",
            "|   12|17-01-1950|  2.8|\n",
            "|   13|18-01-1950|  2.8|\n",
            "|   14|19-01-1950|  2.8|\n",
            "|   15|20-01-1950|  2.8|\n",
            "|   16|23-01-1950|  2.8|\n",
            "|   17|24-01-1950|  2.8|\n",
            "|   18|25-01-1950|  2.8|\n",
            "|   19|26-01-1950|  2.8|\n",
            "|   20|27-01-1950|  2.8|\n",
            "+-----+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12895"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lead\n",
        "# Define the window specification to order the data by \"Index\" (row order)\n",
        "# This will allow us to compute the previous day's value using the lag() function\n",
        "windowSpec = Window.orderBy(\"Index\")\n",
        "\n",
        "# Add a new column \"PrevValue\" which contains the value of the previous day\n",
        "# This is done using lag() function over the \"windowSpec\" which orders the data by \"Index\"\n",
        "df4 = df3.withColumn(\"PrevValue\", lag(\"Value\").over(windowSpec)) # lead kullnaarak da geri çekebilirdim\n",
        "\n",
        "# Add a new column \"PrevDate\" that holds the previous day's \"Date\"\n",
        "df5 = df4.withColumn(\"PrevDate\", lag(\"Date\").over(windowSpec)) # lead kullnaarak da geri çekebilirdim\n",
        "\n",
        "# Show the updated DataFrame to verify the new column\n",
        "df5.show()\n",
        "\n",
        "# Count the total number of rows in the DataFrame\n",
        "df5.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJGkUURrN21o",
        "outputId": "b9a68516-71e4-47d3-e759-f9ddc5ff5203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+---------+----------+\n",
            "|Index|      Date|Value|PrevValue|  PrevDate|\n",
            "+-----+----------+-----+---------+----------+\n",
            "|    1|02-01-1950|  2.8|     NULL|      NULL|\n",
            "|    2|03-01-1950|  2.8|      2.8|02-01-1950|\n",
            "|    3|04-01-1950|  2.8|      2.8|03-01-1950|\n",
            "|    4|05-01-1950|  2.8|      2.8|04-01-1950|\n",
            "|    5|06-01-1950|  2.8|      2.8|05-01-1950|\n",
            "|    6|09-01-1950|  2.8|      2.8|06-01-1950|\n",
            "|    7|10-01-1950|  2.8|      2.8|09-01-1950|\n",
            "|    8|11-01-1950|  2.8|      2.8|10-01-1950|\n",
            "|    9|12-01-1950|  2.8|      2.8|11-01-1950|\n",
            "|   10|13-01-1950|  2.8|      2.8|12-01-1950|\n",
            "|   11|16-01-1950|  2.8|      2.8|13-01-1950|\n",
            "|   12|17-01-1950|  2.8|      2.8|16-01-1950|\n",
            "|   13|18-01-1950|  2.8|      2.8|17-01-1950|\n",
            "|   14|19-01-1950|  2.8|      2.8|18-01-1950|\n",
            "|   15|20-01-1950|  2.8|      2.8|19-01-1950|\n",
            "|   16|23-01-1950|  2.8|      2.8|20-01-1950|\n",
            "|   17|24-01-1950|  2.8|      2.8|23-01-1950|\n",
            "|   18|25-01-1950|  2.8|      2.8|24-01-1950|\n",
            "|   19|26-01-1950|  2.8|      2.8|25-01-1950|\n",
            "|   20|27-01-1950|  2.8|      2.8|26-01-1950|\n",
            "+-----+----------+-----+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12895"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage change between the current value and the previous value\n",
        "# The formula is: (Current Value - Previous Value) / Previous Value * 100\n",
        "# Round the result to 2 decimal places using the round() function\n",
        "df6 = df5.withColumn(\"PctChange\",\n",
        "                    round(((col(\"Value\") - col(\"PrevValue\")) / col(\"PrevValue\")) * 100, 2))\n",
        "\n",
        "# Display the DataFrame with the calculated percentage change\n",
        "df6.show()\n",
        "df6.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrK6AJnTPLY_",
        "outputId": "54fd4f82-b0c1-4e46-ed38-cf2dd1ffdc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+---------+----------+---------+\n",
            "|Index|      Date|Value|PrevValue|  PrevDate|PctChange|\n",
            "+-----+----------+-----+---------+----------+---------+\n",
            "|    1|02-01-1950|  2.8|     NULL|      NULL|     NULL|\n",
            "|    2|03-01-1950|  2.8|      2.8|02-01-1950|      0.0|\n",
            "|    3|04-01-1950|  2.8|      2.8|03-01-1950|      0.0|\n",
            "|    4|05-01-1950|  2.8|      2.8|04-01-1950|      0.0|\n",
            "|    5|06-01-1950|  2.8|      2.8|05-01-1950|      0.0|\n",
            "|    6|09-01-1950|  2.8|      2.8|06-01-1950|      0.0|\n",
            "|    7|10-01-1950|  2.8|      2.8|09-01-1950|      0.0|\n",
            "|    8|11-01-1950|  2.8|      2.8|10-01-1950|      0.0|\n",
            "|    9|12-01-1950|  2.8|      2.8|11-01-1950|      0.0|\n",
            "|   10|13-01-1950|  2.8|      2.8|12-01-1950|      0.0|\n",
            "|   11|16-01-1950|  2.8|      2.8|13-01-1950|      0.0|\n",
            "|   12|17-01-1950|  2.8|      2.8|16-01-1950|      0.0|\n",
            "|   13|18-01-1950|  2.8|      2.8|17-01-1950|      0.0|\n",
            "|   14|19-01-1950|  2.8|      2.8|18-01-1950|      0.0|\n",
            "|   15|20-01-1950|  2.8|      2.8|19-01-1950|      0.0|\n",
            "|   16|23-01-1950|  2.8|      2.8|20-01-1950|      0.0|\n",
            "|   17|24-01-1950|  2.8|      2.8|23-01-1950|      0.0|\n",
            "|   18|25-01-1950|  2.8|      2.8|24-01-1950|      0.0|\n",
            "|   19|26-01-1950|  2.8|      2.8|25-01-1950|      0.0|\n",
            "|   20|27-01-1950|  2.8|      2.8|26-01-1950|      0.0|\n",
            "+-----+----------+-----+---------+----------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12895"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, concat, lit\n",
        "\n",
        "# Create a new column \"ChangeDate\" that combines \"PrevDate\" and \"Date\"\n",
        "# Format: \"from <PrevDate> to <Date>\"\n",
        "df7 = df6.withColumn(\n",
        "    \"ChangeDate\",\n",
        "    concat(lit(\"from \"), col(\"PrevDate\"), lit(\" to \"), col(\"Date\"))\n",
        ")\n",
        "\n",
        "# Display the DataFrame (first 20 rows) to verify the percentage change and date range\n",
        "df7.show(20, False)\n",
        "\n",
        "# Count the total number of rows in the DataFrame\n",
        "df7.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwKDEyQ8nTmG",
        "outputId": "aa2ca05e-e42c-4f9a-c00d-d71a188c76a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+---------+----------+---------+-----------------------------+\n",
            "|Index|Date      |Value|PrevValue|PrevDate  |PctChange|ChangeDate                   |\n",
            "+-----+----------+-----+---------+----------+---------+-----------------------------+\n",
            "|1    |02-01-1950|2.8  |NULL     |NULL      |NULL     |NULL                         |\n",
            "|2    |03-01-1950|2.8  |2.8      |02-01-1950|0.0      |from 02-01-1950 to 03-01-1950|\n",
            "|3    |04-01-1950|2.8  |2.8      |03-01-1950|0.0      |from 03-01-1950 to 04-01-1950|\n",
            "|4    |05-01-1950|2.8  |2.8      |04-01-1950|0.0      |from 04-01-1950 to 05-01-1950|\n",
            "|5    |06-01-1950|2.8  |2.8      |05-01-1950|0.0      |from 05-01-1950 to 06-01-1950|\n",
            "|6    |09-01-1950|2.8  |2.8      |06-01-1950|0.0      |from 06-01-1950 to 09-01-1950|\n",
            "|7    |10-01-1950|2.8  |2.8      |09-01-1950|0.0      |from 09-01-1950 to 10-01-1950|\n",
            "|8    |11-01-1950|2.8  |2.8      |10-01-1950|0.0      |from 10-01-1950 to 11-01-1950|\n",
            "|9    |12-01-1950|2.8  |2.8      |11-01-1950|0.0      |from 11-01-1950 to 12-01-1950|\n",
            "|10   |13-01-1950|2.8  |2.8      |12-01-1950|0.0      |from 12-01-1950 to 13-01-1950|\n",
            "|11   |16-01-1950|2.8  |2.8      |13-01-1950|0.0      |from 13-01-1950 to 16-01-1950|\n",
            "|12   |17-01-1950|2.8  |2.8      |16-01-1950|0.0      |from 16-01-1950 to 17-01-1950|\n",
            "|13   |18-01-1950|2.8  |2.8      |17-01-1950|0.0      |from 17-01-1950 to 18-01-1950|\n",
            "|14   |19-01-1950|2.8  |2.8      |18-01-1950|0.0      |from 18-01-1950 to 19-01-1950|\n",
            "|15   |20-01-1950|2.8  |2.8      |19-01-1950|0.0      |from 19-01-1950 to 20-01-1950|\n",
            "|16   |23-01-1950|2.8  |2.8      |20-01-1950|0.0      |from 20-01-1950 to 23-01-1950|\n",
            "|17   |24-01-1950|2.8  |2.8      |23-01-1950|0.0      |from 23-01-1950 to 24-01-1950|\n",
            "|18   |25-01-1950|2.8  |2.8      |24-01-1950|0.0      |from 24-01-1950 to 25-01-1950|\n",
            "|19   |26-01-1950|2.8  |2.8      |25-01-1950|0.0      |from 25-01-1950 to 26-01-1950|\n",
            "|20   |27-01-1950|2.8  |2.8      |26-01-1950|0.0      |from 26-01-1950 to 27-01-1950|\n",
            "+-----+----------+-----+---------+----------+---------+-----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12895"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by PctChange in descending order to find the top 5 days with the highest percentage increase\n",
        "# Select only the Date and PctChange columns, and limit the result to the top 5\n",
        "top5 = df7.orderBy(col(\"PctChange\").desc()).select(\"ChangeDate\", \"PctChange\").limit(5)\n",
        "\n",
        "# Display the top 5 rows showing the highest percentage increase in \"PctChange\"\n",
        "print(\"top 5 greatest daily increase ( by percentage )\")\n",
        "top5.show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-tymXmxPoi0",
        "outputId": "ac2cdc0f-2e1a-44b6-8db6-1e4c20aa75b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 5 greatest daily increase ( by percentage )\n",
            "+-----------------------------+---------+\n",
            "|ChangeDate                   |PctChange|\n",
            "+-----------------------------+---------+\n",
            "|from 19-08-1960 to 22-08-1960|221.43   |\n",
            "|from 24-01-1980 to 25-01-1980|100.0    |\n",
            "|from 07-08-1970 to 10-08-1970|65.0     |\n",
            "|from 11-06-1979 to 12-06-1979|32.08    |\n",
            "|from 28-02-1978 to 01-03-1978|29.87    |\n",
            "+-----------------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9dl69Ts6GSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}