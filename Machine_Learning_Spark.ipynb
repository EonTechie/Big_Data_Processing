{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EonTechie/Big_Data_Processing/blob/main/Machine_Learning_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LECoCBLjUNip",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LECoCBLjUNip",
        "outputId": "bdfc1139-9cb1-499b-994f-bda757d1c894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['2.txt', 'Capitals.txt', 'EartquakeData-07032025.txt', 'DollarDataset.txt', 'couples.txt', 'join-actors.txt', 'points-null-values.txt', 'numbers-test.txt', 'join-series.txt', 'points.txt', 'names.txt', 'Lottery.txt', 'JamesJoyce-Ulyses.txt', 'world.txt', 'points-places.txt', 'Iris.csv', 'ml-latest-small', 'iris-dataset.txt', 'HousePrices-1.txt', 'HousePrices-2.txt', 'HousePrices-3.txt', 'CombinedHousePricesOutput', 'CombinedHousePricesOutput.txt', 'datasetsoutput_prices', 'house_prices_combined.csv', 'output_prices', 'leaf.csv', 'hello1.txt', 'hello2.txt', 'hello3.txt', 'movie_turkish_train.txt']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/datasets\"\n",
        "files = os.listdir(folder_path)\n",
        "print(files)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this notebook, I applied extensive and diverse hyperparameter tuning for decision tree, random forest and multilayer perceptron classifiers\n",
        "# to each classifier using both TrainValidationSplit and CrossValidator.\n",
        "# To ensure reproducibility, I used fixed random seeds throughout training and splitting.\n",
        "# Due to the large parameter grids and 5-fold cross-validation,\n",
        "# some training sessions took up to 2 hours.\n",
        "# Since it was not feasible to run all models within the same Colab runtime session,\n",
        "# I saved each model's best result to Google Drive and later combined them into a single summary table.\n",
        "\n",
        "# The models can be reached from this drive link: https://drive.google.com/drive/folders/19dl89-bdIc8U5rFM4HUBx5mWSvJfnoHr?usp=sharing\n",
        "# The results can be reached from this drive link: https://drive.google.com/drive/folders/1yJnPVaDa2iLqKbcHi72Tj9q8XWj_bgKj?usp=sharing"
      ],
      "metadata": {
        "id": "XB6iZ-2w4W3c"
      },
      "id": "XB6iZ-2w4W3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kepb7kgS9wvr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kepb7kgS9wvr",
        "outputId": "48146071-6de7-40c6-cb81-91f2ed8fcb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Decision Tree Parameters:\n",
            " cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label, current: Class)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 1119950754009369905, current: 42)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined) \n",
            "\n",
            " Random Forest Parameters:\n",
            " bootstrap: Whether bootstrap samples are used when building trees. (default: True)\n",
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label, current: Class)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "numTrees: Number of trees to train (>= 1). (default: 20)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 5380320264183841840, current: 42)\n",
            "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined) \n",
            "\n",
            " MLP Parameters:\n",
            " blockSize: block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. (default: 128)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "initialWeights: The initial weights of the model. (undefined)\n",
            "labelCol: label column name. (default: label, current: Class)\n",
            "layers: Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons. (undefined)\n",
            "maxIter: max number of iterations (>= 0). (default: 100)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 8663820628711852186, current: 42)\n",
            "solver: The solver algorithm for optimization. Supported options: l-bfgs, gd. (default: l-bfgs)\n",
            "stepSize: Step size to be used for each iteration of optimization (>= 0). (default: 0.03)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Required imports\n",
        "from pyspark.ml.classification import (\n",
        "    DecisionTreeClassifier,\n",
        "    RandomForestClassifier,\n",
        "    MultilayerPerceptronClassifier\n",
        ")\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# Set a fixed seed for reproducibility\n",
        "seed_value = 42\n",
        "\n",
        "# Initialize each classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "mlp = MultilayerPerceptronClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "\n",
        "# Print explainParams for each\n",
        "print(\" Decision Tree Parameters:\\n\", dt.explainParams(), \"\\n\")\n",
        "print(\" Random Forest Parameters:\\n\", rf.explainParams(), \"\\n\")\n",
        "print(\" MLP Parameters:\\n\", mlp.explainParams(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853393f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853393f0",
        "outputId": "46ef67d3-0ea8-4522-d023-356ab516749a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
            "|_c0|_c1|    _c2|   _c3|    _c4|    _c5|    _c6|    _c7|      _c8|      _c9|     _c10|    _c11|     _c12|     _c13|     _c14|   _c15|\n",
            "+---+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
            "|  1|  1|0.72694|1.4742|0.32396|0.98535|    1.0|0.83592|0.0046566|0.0039465|  0.04779| 0.12795| 0.016108|0.0052323|2.7477E-4| 1.1756|\n",
            "|  1|  2|0.74173|1.5257|0.36116|0.98152|0.99825|0.79867|0.0052423|0.0050016|  0.02416|0.090476|0.0081195| 0.002708|7.4846E-5|0.69659|\n",
            "|  1|  3|0.76722|1.5725|0.38998|0.97755|    1.0|0.80812|0.0074573| 0.010121| 0.011897|0.057445|0.0032891|9.2068E-4|3.7886E-5|0.44348|\n",
            "|  1|  4|0.73797|1.4597|0.35376|0.97566|    1.0|0.81697|0.0068768|0.0086068|  0.01595|0.065491|0.0042707|0.0011544|6.6272E-5|0.58785|\n",
            "|  1|  5|0.82301|1.7707|0.44462|0.97698|    1.0|0.75493| 0.007428| 0.010042|0.0079379|0.045339|0.0020514|5.5986E-4|2.3504E-5|0.34214|\n",
            "|  1|  6|0.72997|1.4892|0.34284|0.98755|    1.0|0.84482|0.0049451|0.0044506| 0.010487|0.058528|0.0034138|0.0011248|2.4798E-5|0.34068|\n",
            "|  1|  7|0.82063|1.7529|0.44458|0.97964|0.99649| 0.7677|0.0059279|0.0063954| 0.018375|0.080587|0.0064523|0.0022713|4.1495E-5|0.53904|\n",
            "|  1|  8|0.77982|1.6215|0.39222|0.98512|0.99825|0.80816|0.0050987|0.0047314| 0.024875|0.089686|0.0079794|0.0024664|1.4676E-4|0.66975|\n",
            "|  1|  9|0.83089|1.8199|0.45693| 0.9824|    1.0|0.77106|0.0060055| 0.006564|0.0072447|0.040616|0.0016469|3.8812E-4|3.2863E-5|0.33696|\n",
            "|  1| 10|0.90631|2.3906|0.58336|0.97683|0.99825|0.66419|0.0084019| 0.012848|0.0070096|0.042347|0.0017901|4.5889E-4|2.8251E-5|0.28082|\n",
            "|  1| 11| 0.7459|1.4927|0.34116|0.98296|    1.0|0.83088|0.0055665|0.0056395|0.0057679|0.036511|0.0013313|3.0872E-4|3.1839E-5|0.25026|\n",
            "|  1| 12|0.79606|1.6934|0.43387|0.98181|    1.0|0.76985|0.0077992| 0.011071| 0.013677|0.057832|0.0033334|8.1648E-4|1.3855E-4|0.49751|\n",
            "|  2|  1|0.93361|2.7582|0.64257|0.98346|    1.0|0.59851|0.0055336|0.0055731| 0.029712|0.089889|0.0080153|0.0020648|2.3883E-4|0.91499|\n",
            "|  2|  2|0.91186|2.4994|0.60323|  0.983|    1.0|0.64916|0.0061494|0.0068823| 0.018887|0.072486|0.0052267|0.0014887|8.3271E-5|0.67811|\n",
            "|  2|  3|0.89063|2.2927|0.56667|0.98732|    1.0|0.66427|0.0028365|0.0014643| 0.029272|0.091328|0.0082717|0.0022383|2.0166E-4|0.87177|\n",
            "|  2|  4|0.86755| 2.009|0.51464|0.98691|    1.0|0.70277|0.0054439|0.0053937| 0.030348|0.092063|0.0084044|0.0022541|1.9854E-4|0.94545|\n",
            "|  2|  5|0.91852|2.5247|0.61648| 0.9787|    1.0|0.63037|0.0050494|0.0046404|  0.02309|0.082029|0.0066839|0.0018929|1.2452E-4|0.71713|\n",
            "|  2|  6|0.88795|2.2038|0.56218|0.97835|0.99825|0.64158|0.0059242|0.0063874| 0.032722|0.092969|0.0085691|0.0021199|2.7729E-4|  1.008|\n",
            "|  2|  7|0.85121|1.9548| 0.4892|0.98622|    1.0|0.70267|0.0039733|0.0028733| 0.020258|0.070841|0.0049933|0.0012274|1.4929E-4|0.74174|\n",
            "|  2|  8|0.89084|2.2979|0.57815|0.97389|    1.0|0.64598| 0.015271| 0.042443| 0.028461|0.086477|0.0074228|0.0018832|2.4345E-4|0.91307|\n",
            "+---+---+-------+------+-------+-------+-------+-------+---------+---------+---------+--------+---------+---------+---------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "leafDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"false\").csv(\"/content/drive/My Drive/datasets/leaf.csv\")\n",
        "leafDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32dARgN6Q-Hq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32dARgN6Q-Hq",
        "outputId": "9d76f65b-f83e-4de0-ba90-40726ca904dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+------------+------------+----------+--------+--------------------+--------------------+-------------------------+---------+-----------------+----------------+----------+------------+----------+-------+\n",
            "|Class|Specimen Number|Eccentricity|Aspect Ratio|Elongation|Solidity|Stochastic Convexity|Isoperimetric Factor|Maximal Indentation Depth|Lobedness|Average Intensity|Average Contrast|Smoothness|Third moment|Uniformity|Entropy|\n",
            "+-----+---------------+------------+------------+----------+--------+--------------------+--------------------+-------------------------+---------+-----------------+----------------+----------+------------+----------+-------+\n",
            "|1    |1              |0.72694     |1.4742      |0.32396   |0.98535 |1.0                 |0.83592             |0.0046566                |0.0039465|0.04779          |0.12795         |0.016108  |0.0052323   |2.7477E-4 |1.1756 |\n",
            "|1    |2              |0.74173     |1.5257      |0.36116   |0.98152 |0.99825             |0.79867             |0.0052423                |0.0050016|0.02416          |0.090476        |0.0081195 |0.002708    |7.4846E-5 |0.69659|\n",
            "|1    |3              |0.76722     |1.5725      |0.38998   |0.97755 |1.0                 |0.80812             |0.0074573                |0.010121 |0.011897         |0.057445        |0.0032891 |9.2068E-4   |3.7886E-5 |0.44348|\n",
            "|1    |4              |0.73797     |1.4597      |0.35376   |0.97566 |1.0                 |0.81697             |0.0068768                |0.0086068|0.01595          |0.065491        |0.0042707 |0.0011544   |6.6272E-5 |0.58785|\n",
            "|1    |5              |0.82301     |1.7707      |0.44462   |0.97698 |1.0                 |0.75493             |0.007428                 |0.010042 |0.0079379        |0.045339        |0.0020514 |5.5986E-4   |2.3504E-5 |0.34214|\n",
            "+-----+---------------+------------+------------+----------+--------+--------------------+--------------------+-------------------------+---------+-----------------+----------------+----------+------------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Correct column names based on the dataset description\n",
        "column_names = [\n",
        "    \"Class\", \"Specimen Number\", \"Eccentricity\", \"Aspect Ratio\", \"Elongation\",\n",
        "    \"Solidity\", \"Stochastic Convexity\", \"Isoperimetric Factor\", \"Maximal Indentation Depth\",\n",
        "    \"Lobedness\", \"Average Intensity\", \"Average Contrast\", \"Smoothness\",\n",
        "    \"Third moment\", \"Uniformity\", \"Entropy\"\n",
        "]\n",
        "\n",
        "# Rename all columns in the DataFrame\n",
        "for old, new in zip(leafDF.columns, column_names):\n",
        "    leafDF = leafDF.withColumnRenamed(old, new)\n",
        "\n",
        "# Show renamed columns and first rows\n",
        "leafDF.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YK0-SyqNXY8j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK0-SyqNXY8j",
        "outputId": "e8a15386-6761-4e47-f466-d4a322c33f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|Class|\n",
            "+-----+\n",
            "|    1|\n",
            "|    2|\n",
            "|    3|\n",
            "|    4|\n",
            "|    5|\n",
            "|    6|\n",
            "|    7|\n",
            "|    8|\n",
            "|    9|\n",
            "|   10|\n",
            "|   11|\n",
            "|   12|\n",
            "|   13|\n",
            "|   14|\n",
            "|   15|\n",
            "|   22|\n",
            "|   23|\n",
            "|   24|\n",
            "|   25|\n",
            "|   26|\n",
            "|   27|\n",
            "|   28|\n",
            "|   29|\n",
            "|   30|\n",
            "|   31|\n",
            "|   32|\n",
            "|   33|\n",
            "|   34|\n",
            "|   35|\n",
            "|   36|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "leafDF.select(\"Class\").distinct().orderBy(\"Class\").show(50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "APLIgZz6UxS2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APLIgZz6UxS2",
        "outputId": "bba63c8a-27f9-4fec-bd42-aee27885bcc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+------------+------------+----------+--------+--------------------+--------------------+-------------------------+---------+-----------------+----------------+----------+------------+----------+-------+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Class|Specimen Number|Eccentricity|Aspect Ratio|Elongation|Solidity|Stochastic Convexity|Isoperimetric Factor|Maximal Indentation Depth|Lobedness|Average Intensity|Average Contrast|Smoothness|Third moment|Uniformity|Entropy|features                                                                                                                         |\n",
            "+-----+---------------+------------+------------+----------+--------+--------------------+--------------------+-------------------------+---------+-----------------+----------------+----------+------------+----------+-------+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1    |1              |0.72694     |1.4742      |0.32396   |0.98535 |1.0                 |0.83592             |0.0046566                |0.0039465|0.04779          |0.12795         |0.016108  |0.0052323   |2.7477E-4 |1.1756 |[1.0,0.72694,1.4742,0.32396,0.98535,1.0,0.83592,0.0046566,0.0039465,0.04779,0.12795,0.016108,0.0052323,2.7477E-4,1.1756]         |\n",
            "|1    |2              |0.74173     |1.5257      |0.36116   |0.98152 |0.99825             |0.79867             |0.0052423                |0.0050016|0.02416          |0.090476        |0.0081195 |0.002708    |7.4846E-5 |0.69659|[2.0,0.74173,1.5257,0.36116,0.98152,0.99825,0.79867,0.0052423,0.0050016,0.02416,0.090476,0.0081195,0.002708,7.4846E-5,0.69659]   |\n",
            "|1    |3              |0.76722     |1.5725      |0.38998   |0.97755 |1.0                 |0.80812             |0.0074573                |0.010121 |0.011897         |0.057445        |0.0032891 |9.2068E-4   |3.7886E-5 |0.44348|[3.0,0.76722,1.5725,0.38998,0.97755,1.0,0.80812,0.0074573,0.010121,0.011897,0.057445,0.0032891,9.2068E-4,3.7886E-5,0.44348]      |\n",
            "|1    |4              |0.73797     |1.4597      |0.35376   |0.97566 |1.0                 |0.81697             |0.0068768                |0.0086068|0.01595          |0.065491        |0.0042707 |0.0011544   |6.6272E-5 |0.58785|[4.0,0.73797,1.4597,0.35376,0.97566,1.0,0.81697,0.0068768,0.0086068,0.01595,0.065491,0.0042707,0.0011544,6.6272E-5,0.58785]      |\n",
            "|1    |5              |0.82301     |1.7707      |0.44462   |0.97698 |1.0                 |0.75493             |0.007428                 |0.010042 |0.0079379        |0.045339        |0.0020514 |5.5986E-4   |2.3504E-5 |0.34214|[5.0,0.82301,1.7707,0.44462,0.97698,1.0,0.75493,0.007428,0.010042,0.0079379,0.045339,0.0020514,5.5986E-4,2.3504E-5,0.34214]      |\n",
            "|1    |6              |0.72997     |1.4892      |0.34284   |0.98755 |1.0                 |0.84482             |0.0049451                |0.0044506|0.010487         |0.058528        |0.0034138 |0.0011248   |2.4798E-5 |0.34068|[6.0,0.72997,1.4892,0.34284,0.98755,1.0,0.84482,0.0049451,0.0044506,0.010487,0.058528,0.0034138,0.0011248,2.4798E-5,0.34068]     |\n",
            "|1    |7              |0.82063     |1.7529      |0.44458   |0.97964 |0.99649             |0.7677              |0.0059279                |0.0063954|0.018375         |0.080587        |0.0064523 |0.0022713   |4.1495E-5 |0.53904|[7.0,0.82063,1.7529,0.44458,0.97964,0.99649,0.7677,0.0059279,0.0063954,0.018375,0.080587,0.0064523,0.0022713,4.1495E-5,0.53904]  |\n",
            "|1    |8              |0.77982     |1.6215      |0.39222   |0.98512 |0.99825             |0.80816             |0.0050987                |0.0047314|0.024875         |0.089686        |0.0079794 |0.0024664   |1.4676E-4 |0.66975|[8.0,0.77982,1.6215,0.39222,0.98512,0.99825,0.80816,0.0050987,0.0047314,0.024875,0.089686,0.0079794,0.0024664,1.4676E-4,0.66975] |\n",
            "|1    |9              |0.83089     |1.8199      |0.45693   |0.9824  |1.0                 |0.77106             |0.0060055                |0.006564 |0.0072447        |0.040616        |0.0016469 |3.8812E-4   |3.2863E-5 |0.33696|[9.0,0.83089,1.8199,0.45693,0.9824,1.0,0.77106,0.0060055,0.006564,0.0072447,0.040616,0.0016469,3.8812E-4,3.2863E-5,0.33696]      |\n",
            "|1    |10             |0.90631     |2.3906      |0.58336   |0.97683 |0.99825             |0.66419             |0.0084019                |0.012848 |0.0070096        |0.042347        |0.0017901 |4.5889E-4   |2.8251E-5 |0.28082|[10.0,0.90631,2.3906,0.58336,0.97683,0.99825,0.66419,0.0084019,0.012848,0.0070096,0.042347,0.0017901,4.5889E-4,2.8251E-5,0.28082]|\n",
            "+-----+---------------+------------+------------+----------+--------+--------------------+--------------------+-------------------------+---------+-----------------+----------------+----------+------------+----------+-------+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List of feature column names with proper spacing and casing\n",
        "feature_cols = [\n",
        "    \"Specimen Number\", \"Eccentricity\", \"Aspect Ratio\", \"Elongation\",\n",
        "    \"Solidity\", \"Stochastic Convexity\", \"Isoperimetric Factor\", \"Maximal Indentation Depth\",\n",
        "    \"Lobedness\", \"Average Intensity\", \"Average Contrast\", \"Smoothness\",\n",
        "    \"Third moment\", \"Uniformity\", \"Entropy\"\n",
        "]\n",
        "\n",
        "# Create a VectorAssembler to combine all feature columns into a single 'features' column\n",
        "vec = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Transform the original DataFrame to add the 'features' column\n",
        "leafDF = vec.transform(leafDF)\n",
        "leafDF.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pdE8sN1eMYkC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdE8sN1eMYkC",
        "outputId": "66dc7ef6-e34a-439f-9769-23d14a088500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|features                                                                                                                      |Class|\n",
            "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|[1.0,0.72694,1.4742,0.32396,0.98535,1.0,0.83592,0.0046566,0.0039465,0.04779,0.12795,0.016108,0.0052323,2.7477E-4,1.1756]      |1    |\n",
            "|[2.0,0.74173,1.5257,0.36116,0.98152,0.99825,0.79867,0.0052423,0.0050016,0.02416,0.090476,0.0081195,0.002708,7.4846E-5,0.69659]|1    |\n",
            "|[3.0,0.76722,1.5725,0.38998,0.97755,1.0,0.80812,0.0074573,0.010121,0.011897,0.057445,0.0032891,9.2068E-4,3.7886E-5,0.44348]   |1    |\n",
            "|[4.0,0.73797,1.4597,0.35376,0.97566,1.0,0.81697,0.0068768,0.0086068,0.01595,0.065491,0.0042707,0.0011544,6.6272E-5,0.58785]   |1    |\n",
            "|[5.0,0.82301,1.7707,0.44462,0.97698,1.0,0.75493,0.007428,0.010042,0.0079379,0.045339,0.0020514,5.5986E-4,2.3504E-5,0.34214]   |1    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show sample rows with class labels and generated features\n",
        "leafDF.select(\"features\",\"Class\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SWQF2u2m9bwq",
      "metadata": {
        "id": "SWQF2u2m9bwq"
      },
      "outputs": [],
      "source": [
        "############################################################################################ DECISION TREE CLASSIFIER ###############################################################################################\n",
        "############################################################################################ DECISION TREE CLASSIFIER ###############################################################################################\n",
        "############################################################################################ DECISION TREE CLASSIFIER ###############################################################################################\n",
        "############################################################################################ DECISION TREE CLASSIFIER ###############################################################################################\n",
        "############################################################################################ DECISION TREE CLASSIFIER ###############################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier with TrainValidationSplit"
      ],
      "metadata": {
        "id": "xg6dv1sR5t6t"
      },
      "id": "xg6dv1sR5t6t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NB36u7RPfg3L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB36u7RPfg3L",
        "outputId": "d0a89c1f-201e-43e1-acab-aba0793c4bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 288\n",
            "Test set size: 52\n",
            "+--------------------+-----+----------+\n",
            "|            features|Class|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[1.0,0.4132,1.038...|   15|      15.0|\n",
            "|[1.0,0.50924,1.21...|   30|      30.0|\n",
            "|[1.0,0.60267,1.25...|   27|      26.0|\n",
            "|[1.0,0.71763,1.50...|   13|       1.0|\n",
            "|[1.0,0.86224,2.07...|   32|       7.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy (Decision Tree with TVS): 0.6538461538461539\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree Classifier with TrainValidationSplit and comprehensive parameter tuning\n",
        "# This configuration includes all key hyperparameters relevant to the leaf dataset,\n",
        "# which contains 340 samples across 40 classes (most with ~10 instances).\n",
        "# A fixed seed is used for reproducibility across splitting and training.\n",
        "# Parameters were selected based on their influence on training behavior and model complexity.\n",
        "# Visualization-only or output-only parameters (e.g., thresholds, leafCol, probabilityCol) were intentionally excluded to avoid unnecessary tuning overhead.\n",
        "\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Set seed value for reproducibility\n",
        "seed_value = 42\n",
        "\n",
        "# Prepare dataset and perform stratified split\n",
        "leafDF = leafDF.select(\"features\", \"Class\")\n",
        "trainDF, testDF = leafDF.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "\n",
        "# Optional: Print class balance check\n",
        "print(\"Training set size:\", trainDF.count())\n",
        "print(\"Test set size:\", testDF.count())\n",
        "\n",
        "# Initialize classifier with seed\n",
        "dt = DecisionTreeClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "paramGrid_dt = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(dt.maxDepth, [3, 5, 7, 10])                 # Controls tree complexity to prevent overfitting\n",
        "    .addGrid(dt.impurity, [\"gini\", \"entropy\"])           # Split criterion: Gini vs. Entropy\n",
        "    .addGrid(dt.maxBins, [32, 64])                       # Number of bins used for feature discretization\n",
        "    .addGrid(dt.minInstancesPerNode, [1, 2])             # Minimum number of instances per child after split\n",
        "    .addGrid(dt.minInfoGain, [0.0, 0.01])                # Discards splits with low information gain\n",
        "    .addGrid(dt.minWeightFractionPerNode, [0.0])         # Ensures minimum relative weight in child nodes (default 0.0)\n",
        "    .addGrid(dt.maxMemoryInMB, [256, 512])               # Memory budget for training histograms\n",
        "    .addGrid(dt.cacheNodeIds, [True, False])             # Caches node IDs to accelerate deeper trees\n",
        "    .addGrid(dt.checkpointInterval, [10])                # Used for fault-tolerant recovery during long training\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Evaluation metric\n",
        "evaluator_dt = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# TrainValidationSplit setup\n",
        "val_dt = TrainValidationSplit(\n",
        "    estimator=dt,\n",
        "    estimatorParamMaps=paramGrid_dt,\n",
        "    evaluator=evaluator_dt,\n",
        "    trainRatio=0.8,\n",
        "    seed=seed_value\n",
        ")\n",
        "\n",
        "# Train model\n",
        "tuned_model_dt = val_dt.fit(trainDF)\n",
        "bestModel = tuned_model_dt.bestModel\n",
        "\n",
        "# Extract best parameter values as string for later tabulation\n",
        "params_dt_tvs = f\"maxDepth={bestModel.getMaxDepth()}, impurity={bestModel.getImpurity()}, maxBins={bestModel.getMaxBins()}, \" \\\n",
        "                f\"minInstancesPerNode={bestModel.getMinInstancesPerNode()}, minInfoGain={bestModel.getMinInfoGain()}, \" \\\n",
        "                f\"minWeightFractionPerNode={bestModel.getMinWeightFractionPerNode()}, maxMemoryInMB={bestModel.getMaxMemoryInMB()}, \" \\\n",
        "                f\"cacheNodeIds={bestModel.getCacheNodeIds()}, checkpointInterval={bestModel.getCheckpointInterval()}\"\n",
        "\n",
        "# Predict on test set and evaluate\n",
        "resultDF_dt = tuned_model_dt.transform(testDF)\n",
        "resultDF_dt.select(\"features\", \"Class\", \"prediction\").show(5)\n",
        "\n",
        "accuracy_dt_tvs = evaluator_dt.evaluate(resultDF_dt)\n",
        "print(\"Accuracy (Decision Tree with TVS):\", accuracy_dt_tvs)\n",
        "\n",
        "# One-row result object for final result table\n",
        "result_row_dt_tvs = Row(Method=\"Decision Tree (TVS)\", Parameters=params_dt_tvs, Accuracy=accuracy_dt_tvs)\n",
        "spark.createDataFrame([result_row_dt_tvs]).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/results/result_dt_tvs.csv\")\n",
        "tuned_model_dt.bestModel.save('/content/drive/MyDrive/models/tuned_model_dt') # Save best model for DT with TrainValidationSplit Validator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier with CrossValidator"
      ],
      "metadata": {
        "id": "OogRog205wxd"
      },
      "id": "OogRog205wxd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z_xHpm1hc1Ru",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_xHpm1hc1Ru",
        "outputId": "9df42390-0aa1-4ef1-ccad-e38564e895a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 288\n",
            "Test set size: 52\n",
            "+--------------------+-----+----------+\n",
            "|            features|Class|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[1.0,0.4132,1.038...|   15|       6.0|\n",
            "|[1.0,0.50924,1.21...|   30|      30.0|\n",
            "|[1.0,0.60267,1.25...|   27|      24.0|\n",
            "|[1.0,0.71763,1.50...|   13|       1.0|\n",
            "|[1.0,0.86224,2.07...|   32|       2.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy (Decision Tree with CrossValidator): 0.5769230769230769\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree Classifier with CrossValidator and full hyperparameter tuning\n",
        "# This configuration uses 5-fold cross-validation to ensure stable and robust parameter selection.\n",
        "# The leaf dataset has 340 samples across 40 classes; most classes have ~10 samples.\n",
        "# 5 folds maintain a balance between training size and validation reliability for such a small, multi-class dataset.\n",
        "# Parameters were selected by analyzing model-relevant hyperparameters from explainParams().\n",
        "# Parameters not affecting training (like probabilityCol, leafCol, thresholds, etc.) were excluded to keep the search space efficient.\n",
        "\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_value = 42\n",
        "\n",
        "# Prepare data\n",
        "leafDF = leafDF.select(\"features\", \"Class\")\n",
        "trainDF, testDF = leafDF.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "print(\"Training set size:\", trainDF.count())\n",
        "print(\"Test set size:\", testDF.count())\n",
        "\n",
        "# Initialize classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "paramGrid_dt = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(dt.maxDepth, [3, 5, 7, 10])                 # Prevents overfitting by controlling tree complexity\n",
        "    .addGrid(dt.impurity, [\"gini\", \"entropy\"])           # Information gain criterion for node splitting\n",
        "    .addGrid(dt.maxBins, [32, 64])                       # Number of bins used to discretize continuous features\n",
        "    .addGrid(dt.minInstancesPerNode, [1, 2])             # Avoids splits that leave very few samples in children\n",
        "    .addGrid(dt.minInfoGain, [0.0, 0.01])                # Prevents weak or meaningless splits\n",
        "    .addGrid(dt.minWeightFractionPerNode, [0.0])         # Adds regularization based on sample weight proportions\n",
        "    .addGrid(dt.maxMemoryInMB, [256, 512])               # Memory budget for histogram aggregation\n",
        "    .addGrid(dt.cacheNodeIds, [True, False])             # Optionally caches node IDs for performance\n",
        "    .addGrid(dt.checkpointInterval, [10])                # Checkpointing for fault tolerance in long pipelines\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Accuracy-based evaluator for multi-class classification\n",
        "evaluator_dt = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# CrossValidator setup with 5 folds and fixed seed\n",
        "crossval_dt = CrossValidator(\n",
        "    estimator=dt,\n",
        "    estimatorParamMaps=paramGrid_dt,\n",
        "    evaluator=evaluator_dt,\n",
        "    numFolds=5,\n",
        "    seed=seed_value\n",
        ")\n",
        "\n",
        "# Train model using cross-validation\n",
        "cv_model_dt = crossval_dt.fit(trainDF)\n",
        "bestModel_cv = cv_model_dt.bestModel\n",
        "\n",
        "# Extract best parameters into string format for table\n",
        "params_dt_cv = f\"maxDepth={bestModel_cv.getMaxDepth()}, impurity={bestModel_cv.getImpurity()}, maxBins={bestModel_cv.getMaxBins()}, \" \\\n",
        "               f\"minInstancesPerNode={bestModel_cv.getMinInstancesPerNode()}, minInfoGain={bestModel_cv.getMinInfoGain()}, \" \\\n",
        "               f\"minWeightFractionPerNode={bestModel_cv.getMinWeightFractionPerNode()}, maxMemoryInMB={bestModel_cv.getMaxMemoryInMB()}, \" \\\n",
        "               f\"cacheNodeIds={bestModel_cv.getCacheNodeIds()}, checkpointInterval={bestModel_cv.getCheckpointInterval()}\"\n",
        "\n",
        "# Evaluate on test data\n",
        "resultDF_cv = cv_model_dt.transform(testDF)\n",
        "resultDF_cv.select(\"features\", \"Class\", \"prediction\").show(5)\n",
        "\n",
        "accuracy_dt_cv = evaluator_dt.evaluate(resultDF_cv)\n",
        "print(\"Accuracy (Decision Tree with CrossValidator):\", accuracy_dt_cv)\n",
        "\n",
        "# Save result row for final table\n",
        "result_row_dt_cv = Row(Method=\"Decision Tree (CV)\", Parameters=params_dt_cv, Accuracy=accuracy_dt_cv)\n",
        "spark.createDataFrame([result_row_dt_cv]).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/results/result_dt_cv.csv\")\n",
        "cv_model_dt.bestModel.save('/content/drive/MyDrive/models/cv_model_dt') # Save best model for DT with Cross Validation Validator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CKfzThiRUsa8",
      "metadata": {
        "id": "CKfzThiRUsa8"
      },
      "outputs": [],
      "source": [
        "##################################################################################### RANDOM FOREST CLASSIFIER #####################################################################################\n",
        "##################################################################################### RANDOM FOREST CLASSIFIER #####################################################################################\n",
        "##################################################################################### RANDOM FOREST CLASSIFIER #####################################################################################\n",
        "##################################################################################### RANDOM FOREST CLASSIFIER #####################################################################################\n",
        "##################################################################################### RANDOM FOREST CLASSIFIER #####################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier with TrainValidationSplit"
      ],
      "metadata": {
        "id": "TXD3YuI84ekT"
      },
      "id": "TXD3YuI84ekT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13HRTXUHVKji",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13HRTXUHVKji",
        "outputId": "51cc4785-242b-471d-8075-c4cb73ce2ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 288\n",
            "Test set size: 52\n",
            "+--------------------+-----+----------+\n",
            "|            features|Class|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[1.0,0.4132,1.038...|   15|      15.0|\n",
            "|[1.0,0.50924,1.21...|   30|      30.0|\n",
            "|[1.0,0.60267,1.25...|   27|      27.0|\n",
            "|[1.0,0.71763,1.50...|   13|      13.0|\n",
            "|[1.0,0.86224,2.07...|   32|      32.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy (Random Forest with TVS): 0.7115384615384616\n"
          ]
        }
      ],
      "source": [
        "# Random Forest Classifier with TrainValidationSplit and full hyperparameter tuning\n",
        "# This model performs hyperparameter tuning using an explicit grid focused on parameters that directly influence training and generalization.\n",
        "# The leaf dataset contains 340 samples across 40 classes, with ~10 samples per class on average.\n",
        "# We exclude parameters such as probabilityCol, leafCol, thresholds, and rawPredictionCol as they do not influence training behavior.\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Set seed value for reproducibility\n",
        "seed_value = 42\n",
        "\n",
        "# Load and prepare dataset\n",
        "leafDF = leafDF.select(\"features\", \"Class\")\n",
        "trainDF, testDF = leafDF.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "\n",
        "print(\"Training set size:\", trainDF.count())\n",
        "print(\"Test set size:\", testDF.count())\n",
        "\n",
        "# Initialize classifier with seed\n",
        "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "\n",
        "# Define hyperparameter grid based on training-impactful params\n",
        "paramGrid_rf = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(rf.numTrees, [10, 20, 30])                        # numTrees: More trees may improve stability but increase training cost\n",
        "    .addGrid(rf.featureSubsetStrategy, [\"auto\", \"sqrt\"])      # featureSubsetStrategy: Standard values for classification forests\n",
        "    .addGrid(rf.maxDepth, [5, 10])                             # maxDepth: Limited to prevent overfitting on small dataset\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"])                # impurity: Information gain criteria\n",
        "    .addGrid(rf.maxBins, [32, 64])                             # maxBins: Discretization resolution for continuous features\n",
        "    .addGrid(rf.minInstancesPerNode, [1, 2])                   # minInstancesPerNode: Avoids shallow splits with very small subsets\n",
        "    .addGrid(rf.minInfoGain, [0.0, 0.01])                      # minInfoGain: Prevents meaningless or low-gain splits\n",
        "    .addGrid(rf.maxMemoryInMB, [256, 512])                     # maxMemoryInMB: Controls training buffer allocation\n",
        "    .addGrid(rf.cacheNodeIds, [True, False])                   # cacheNodeIds: May speed up repeated access to node paths\n",
        "    .addGrid(rf.subsamplingRate, [0.8, 1.0])                   # subsamplingRate: Tests bootstrap sampling vs full dataset use\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Define evaluator based on classification accuracy\n",
        "evaluator_rf = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Setup TVS\n",
        "tvs_rf = TrainValidationSplit(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid_rf,\n",
        "    evaluator=evaluator_rf,\n",
        "    trainRatio=0.8,\n",
        "    seed=seed_value\n",
        ")\n",
        "\n",
        "# Train model\n",
        "tuned_model_rf = tvs_rf.fit(trainDF)\n",
        "bestModel_rf = tuned_model_rf.bestModel\n",
        "\n",
        "# Extract best parameters as string for report table\n",
        "params_rf_tvs = f\"numTrees={bestModel_rf.getNumTrees}, featureSubsetStrategy={bestModel_rf.getFeatureSubsetStrategy()}, \" \\\n",
        "                f\"maxDepth={bestModel_rf.getMaxDepth()}, impurity={bestModel_rf.getImpurity()}, \" \\\n",
        "                f\"maxBins={bestModel_rf.getMaxBins()}, minInstancesPerNode={bestModel_rf.getMinInstancesPerNode()}, \" \\\n",
        "                f\"minInfoGain={bestModel_rf.getMinInfoGain()}, maxMemoryInMB={bestModel_rf.getMaxMemoryInMB()}, \" \\\n",
        "                f\"cacheNodeIds={bestModel_rf.getCacheNodeIds()}, subsamplingRate={bestModel_rf.getSubsamplingRate()}\"\n",
        "\n",
        "# Predict and evaluate\n",
        "resultDF_rf = tuned_model_rf.transform(testDF)\n",
        "resultDF_rf.select(\"features\", \"Class\", \"prediction\").show(5)\n",
        "\n",
        "accuracy_rf_tvs = evaluator_rf.evaluate(resultDF_rf)\n",
        "print(\"Accuracy (Random Forest with TVS):\", accuracy_rf_tvs)\n",
        "\n",
        "# Format for final result table\n",
        "result_row_rf_tvs = Row(Method=\"Random Forest (TVS)\", Parameters=params_rf_tvs, Accuracy=accuracy_rf_tvs)\n",
        "spark.createDataFrame([result_row_rf_tvs]).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/results/result_rf_tvs.csv\")\n",
        "bestModel_rf.save('/content/drive/MyDrive/models/bestModel_rf')  # Save best model for RF with TrainValidationSplit Validator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier with CrossValidator"
      ],
      "metadata": {
        "id": "ho7BmkfN4lFj"
      },
      "id": "ho7BmkfN4lFj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nxCjGWs53tFi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxCjGWs53tFi",
        "outputId": "96ec415a-e60d-4a6c-8c22-cd0d023ec760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 288\n",
            "Test set size: 52\n",
            "+--------------------+-----+----------+\n",
            "|            features|Class|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[1.0,0.4132,1.038...|   15|      15.0|\n",
            "|[1.0,0.50924,1.21...|   30|      30.0|\n",
            "|[1.0,0.60267,1.25...|   27|      24.0|\n",
            "|[1.0,0.71763,1.50...|   13|      13.0|\n",
            "|[1.0,0.86224,2.07...|   32|      32.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy (Random Forest with CrossValidator): 0.6730769230769231\n"
          ]
        }
      ],
      "source": [
        "# Random Forest Classifier with CrossValidator and full hyperparameter tuning\n",
        "# This version uses 5-fold cross-validation to achieve more robust and stable model selection.\n",
        "# The leaf dataset consists of 340 samples over 40 classes, with many classes having ~10 samples.\n",
        "# Parameters selected below affect training behavior, model complexity, and generalization.\n",
        "# Parameters such as thresholds, probabilityCol, rawPredictionCol, and leafCol were excluded because they affect only post-training output or are not relevant to tuning.\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_value = 42\n",
        "\n",
        "# Prepare data\n",
        "leafDF = leafDF.select(\"features\", \"Class\")\n",
        "trainDF, testDF = leafDF.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "trainDF.cache()\n",
        "trainDF.count()  # triggers caching\n",
        "\n",
        "print(\"Training set size:\", trainDF.count())\n",
        "print(\"Test set size:\", testDF.count())\n",
        "\n",
        "# Initialize classifier\n",
        "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "\n",
        "# Define parameter grid (only training-relevant hyperparameters)\n",
        "paramGrid_rf = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(rf.numTrees, [10, 20, 30])                          # numTrees: Controls ensemble size; more trees increase stability but cost more\n",
        "    .addGrid(rf.featureSubsetStrategy, [\"auto\", \"sqrt\"])        # featureSubsetStrategy: Common defaults for classification; sqrt helps regularize splits\n",
        "    .addGrid(rf.maxDepth, [5, 10])                               # maxDepth: Controls tree complexity to avoid overfitting on small dataset\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"])                  # impurity: Evaluates both splitting criteria\n",
        "    .addGrid(rf.maxBins, [32, 64])                               # maxBins: Higher resolution for continuous variables; trade-off with speed\n",
        "    .addGrid(rf.minInstancesPerNode, [1, 2])                     # minInstancesPerNode: Ensures children have enough data to avoid unstable splits\n",
        "    .addGrid(rf.minInfoGain, [0.0, 0.01])                        # minInfoGain: Forces splits to have meaningful improvement\n",
        "    .addGrid(rf.maxMemoryInMB, [256, 512])                       # maxMemoryInMB: Memory budget for node histogram aggregation\n",
        "    .addGrid(rf.cacheNodeIds, [False])                           # cacheNodeIds: Might improve performance by reusing node paths\n",
        "    .addGrid(rf.subsamplingRate, [0.8, 1.0])                     # subsamplingRate: Tests effect of bootstrap-style sampling vs full dataset\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Accuracy-based evaluator\n",
        "evaluator_rf = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# CrossValidator setup\n",
        "crossval_rf = CrossValidator(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid_rf,\n",
        "    evaluator=evaluator_rf,\n",
        "    numFolds=5,         # 5-fold: Good trade-off between validation reliability and training set size\n",
        "    seed=seed_value,\n",
        "    parallelism=4       # Parallel execution to speed up tuning\n",
        ")\n",
        "\n",
        "# Train model\n",
        "cv_model_rf = crossval_rf.fit(trainDF)\n",
        "bestModel_rf = cv_model_rf.bestModel\n",
        "\n",
        "# Extract best parameter combination\n",
        "params_rf_cv = f\"numTrees={bestModel_rf.getNumTrees}, featureSubsetStrategy={bestModel_rf.getFeatureSubsetStrategy()}, \" \\\n",
        "               f\"maxDepth={bestModel_rf.getMaxDepth()}, impurity={bestModel_rf.getImpurity()}, \" \\\n",
        "               f\"maxBins={bestModel_rf.getMaxBins()}, minInstancesPerNode={bestModel_rf.getMinInstancesPerNode()}, \" \\\n",
        "               f\"minInfoGain={bestModel_rf.getMinInfoGain()}, maxMemoryInMB={bestModel_rf.getMaxMemoryInMB()}, \" \\\n",
        "               f\"cacheNodeIds={bestModel_rf.getCacheNodeIds()}, subsamplingRate={bestModel_rf.getSubsamplingRate()}\"\n",
        "\n",
        "# Predict on test set\n",
        "resultDF_rf = cv_model_rf.transform(testDF)\n",
        "resultDF_rf.select(\"features\", \"Class\", \"prediction\").show(5)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_rf_cv = evaluator_rf.evaluate(resultDF_rf)\n",
        "print(\"Accuracy (Random Forest with CrossValidator):\", accuracy_rf_cv)\n",
        "\n",
        "# Save as result row for final table\n",
        "result_row_rf_cv = Row(Method=\"Random Forest (CV)\", Parameters=params_rf_cv, Accuracy=accuracy_rf_cv)\n",
        "\n",
        "cv_model_rf.bestModel.save('/content/drive/MyDrive/models/cv_model_rf_bestModel')   # Save best model for RF with Cross Validation Validator\n",
        "\n",
        "spark.createDataFrame([result_row_rf_cv]).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/results/result_row_rf_cv.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ESDmgYKUaRkc",
      "metadata": {
        "id": "ESDmgYKUaRkc"
      },
      "outputs": [],
      "source": [
        "##################################################################### MULTILAYER PERCEPTRON CLASSIFIER ###############################################################\n",
        "##################################################################### MULTILAYER PERCEPTRON CLASSIFIER ###############################################################\n",
        "##################################################################### MULTILAYER PERCEPTRON CLASSIFIER ###############################################################\n",
        "##################################################################### MULTILAYER PERCEPTRON CLASSIFIER ###############################################################\n",
        "##################################################################### MULTILAYER PERCEPTRON CLASSIFIER ###############################################################\n",
        "##################################################################### MULTILAYER PERCEPTRON CLASSIFIER ###############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multilayer Perceptron Classifier with TrainValidationSplit"
      ],
      "metadata": {
        "id": "k1ISopg_4m2o"
      },
      "id": "k1ISopg_4m2o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j-7hucxudJe3",
      "metadata": {
        "id": "j-7hucxudJe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10db294d-f1a0-4e53-804b-88974039fa2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 288\n",
            "Test set size: 52\n",
            "+--------------------+-----+----------+\n",
            "|            features|Class|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[1.0,0.4132,1.038...|   15|      36.0|\n",
            "|[1.0,0.50924,1.21...|   30|       9.0|\n",
            "|[1.0,0.60267,1.25...|   27|      24.0|\n",
            "|[1.0,0.71763,1.50...|   13|      33.0|\n",
            "|[1.0,0.86224,2.07...|   32|       2.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy (MLP with TrainValidationSplit): 0.4807692307692308\n"
          ]
        }
      ],
      "source": [
        "# Multilayer Perceptron Classifier with TrainValidationSplit and targeted hyperparameter tuning\n",
        "# This configuration is designed for the leaf dataset, which has only 340 samples across 40 classes (sparse multi-class setup).\n",
        "# To avoid overfitting and long training times, we carefully select meaningful parameters and keep the grid small but effective.\n",
        "# Parameters such as probabilityCol, rawPredictionCol, thresholds, and initialWeights are excluded because they don't affect training behavior or are rarely used in tuning.\n",
        "\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_value = 42\n",
        "\n",
        "# Prepare dataset\n",
        "leafDF = leafDF.select(\"features\", \"Class\")\n",
        "trainDF, testDF = leafDF.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "\n",
        "print(\"Training set size:\", trainDF.count())\n",
        "print(\"Test set size:\", testDF.count())\n",
        "\n",
        "# Define classifier\n",
        "mlp = MultilayerPerceptronClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "\n",
        "# Define network architectures\n",
        "# Since input has 16 features and output has 40 classes, we try 1 shallow and 1 moderately deep network\n",
        "layers_1 = [15, 32, 40]        # One hidden layer\n",
        "layers_2 = [15, 32, 16, 40]    # Two hidden layers\n",
        "\n",
        "# Define parameter grid\n",
        "paramGrid_mlp = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(mlp.layers, [layers_1, layers_2])              # layers: Balanced depth for small dataset; deeper model risks overfitting\n",
        "    .addGrid(mlp.blockSize, [64, 128])                      # blockSize: Mini-batch sizes to test different training granularity\n",
        "    .addGrid(mlp.maxIter, [100, 200])                       # maxIter: Test early stopping vs. longer training\n",
        "    .addGrid(mlp.stepSize, [0.03, 0.1])                     # stepSize: Smaller step for stable convergence, larger to speed up learning\n",
        "    .addGrid(mlp.tol, [1e-4, 1e-6])                         # tol: Test both default and stricter convergence criteria\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Define evaluator\n",
        "evaluator_mlp = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# TrainValidationSplit setup\n",
        "tvs_mlp = TrainValidationSplit(\n",
        "    estimator=mlp,\n",
        "    estimatorParamMaps=paramGrid_mlp,\n",
        "    evaluator=evaluator_mlp,\n",
        "    trainRatio=0.8,\n",
        "    seed=seed_value\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "tuned_model_mlp = tvs_mlp.fit(trainDF)\n",
        "bestModel = tuned_model_mlp.bestModel\n",
        "\n",
        "# Extract best parameters\n",
        "params_mlp_tvs = f\"layers={bestModel.getLayers()}, blockSize={bestModel.getBlockSize()}, \" \\\n",
        "                 f\"maxIter={bestModel.getMaxIter()}, stepSize={bestModel.getStepSize()}, tol={bestModel.getTol()}\"\n",
        "\n",
        "# Evaluate and print results\n",
        "resultDF = tuned_model_mlp.transform(testDF)\n",
        "resultDF.select(\"features\", \"Class\", \"prediction\").show(5)\n",
        "\n",
        "accuracy_mlp_tvs = evaluator_mlp.evaluate(resultDF)\n",
        "print(\"Accuracy (MLP with TrainValidationSplit):\", accuracy_mlp_tvs)\n",
        "\n",
        "# For final result table\n",
        "result_row_mlp_tvs = Row(Method=\"MLP (TVS)\", Parameters=params_mlp_tvs, Accuracy=accuracy_mlp_tvs)\n",
        "# Multilayer Perceptron\n",
        "tuned_model_mlp.bestModel.save('/content/drive/MyDrive/models/tuned_model_mlp_bestModel')  # Save best model for MLP with TrainValidationSplit Validator\n",
        "spark.createDataFrame([result_row_mlp_tvs]).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/results/result_row_mlp_tvs.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multilayer Perceptron Classifier with CrossValidator"
      ],
      "metadata": {
        "id": "QI5jzCae4qj1"
      },
      "id": "QI5jzCae4qj1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jFo5je-C5WC_",
      "metadata": {
        "id": "jFo5je-C5WC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8b5ac6-8d4c-410b-e8ab-4dd4d4a43365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 288\n",
            "Test set size: 52\n",
            "+--------------------+-----+----------+\n",
            "|            features|Class|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[1.0,0.4132,1.038...|   15|      36.0|\n",
            "|[1.0,0.50924,1.21...|   30|       9.0|\n",
            "|[1.0,0.60267,1.25...|   27|       1.0|\n",
            "|[1.0,0.71763,1.50...|   13|      33.0|\n",
            "|[1.0,0.86224,2.07...|   32|       2.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy (MLP with CrossValidator): 0.5\n"
          ]
        }
      ],
      "source": [
        "# Multilayer Perceptron Classifier with CrossValidator and carefully selected hyperparameter tuning\n",
        "# This version uses 5-fold cross-validation for robust model evaluation on the leaf dataset (340 samples, 40 classes).\n",
        "# Cross-validation increases training cost, so the tuning grid is kept minimal but meaningful.\n",
        "# Parameters such as probabilityCol, rawPredictionCol, thresholds, initialWeights, and solver are excluded,\n",
        "# as they do not affect learning performance or are not practical for tuning in this context.\n",
        "\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_value = 42\n",
        "\n",
        "# Prepare dataset\n",
        "leafDF = leafDF.select(\"features\", \"Class\")\n",
        "trainDF, testDF = leafDF.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "\n",
        "print(\"Training set size:\", trainDF.count())\n",
        "print(\"Test set size:\", testDF.count())\n",
        "\n",
        "# Initialize classifier\n",
        "mlp = MultilayerPerceptronClassifier(labelCol=\"Class\", featuresCol=\"features\", seed=seed_value)\n",
        "\n",
        "# Define two network architectures\n",
        "# Due to high number of classes and low sample count, we test one simple and one moderately deep topology\n",
        "layers_1 = [15, 32, 40]         # One hidden layer\n",
        "layers_2 = [15, 32, 16, 40]     # Two hidden layers\n",
        "\n",
        "# Define hyperparameter grid\n",
        "paramGrid_mlp = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(mlp.layers, [layers_1, layers_2])              # layers: Light vs deeper model to test complexity effect\n",
        "    .addGrid(mlp.maxIter, [100])                            # maxIter: Single value to reduce crossval duration\n",
        "    .addGrid(mlp.stepSize, [0.03])                          # stepSize: Fixed small step to ensure convergence\n",
        "    .addGrid(mlp.tol, [1e-4])                               # tol: Default convergence threshold\n",
        "    .addGrid(mlp.blockSize, [64])                           # blockSize: Mini-batch size kept fixed for reproducibility\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Accuracy evaluator\n",
        "evaluator_mlp = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# CrossValidator setup\n",
        "crossval_mlp = CrossValidator(\n",
        "    estimator=mlp,\n",
        "    estimatorParamMaps=paramGrid_mlp,\n",
        "    evaluator=evaluator_mlp,\n",
        "    numFolds=5,          # 5 folds = good trade-off between stability and training data coverage\n",
        "    seed=seed_value\n",
        ")\n",
        "\n",
        "# Train model\n",
        "cv_model_mlp = crossval_mlp.fit(trainDF)\n",
        "bestModel = cv_model_mlp.bestModel\n",
        "\n",
        "# Save best parameters as string\n",
        "params_mlp_cv = f\"layers={bestModel.getLayers()}, blockSize={bestModel.getBlockSize()}, \" \\\n",
        "                f\"maxIter={bestModel.getMaxIter()}, stepSize={bestModel.getStepSize()}, tol={bestModel.getTol()}\"\n",
        "\n",
        "# Evaluate on test set\n",
        "resultDF = cv_model_mlp.transform(testDF)\n",
        "resultDF.select(\"features\", \"Class\", \"prediction\").show(5)\n",
        "\n",
        "accuracy_mlp_cv = evaluator_mlp.evaluate(resultDF)\n",
        "print(\"Accuracy (MLP with CrossValidator):\", accuracy_mlp_cv)\n",
        "\n",
        "# Store for final summary table\n",
        "result_row_mlp_cv = Row(Method=\"MLP (CV)\", Parameters=params_mlp_cv, Accuracy=accuracy_mlp_cv)\n",
        "cv_model_mlp.bestModel.save('/content/drive/MyDrive/models/cv_model_mlp_bestModel')  # Save best model for MLP with Cross Validation Validator\n",
        "\n",
        "spark.createDataFrame([result_row_mlp_cv]).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/results/result_row_mlp_cv.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RcK8R1ZS6mk0",
      "metadata": {
        "id": "RcK8R1ZS6mk0"
      },
      "outputs": [],
      "source": [
        "################################################################################ REPORT CREATING THE TABLE ##########################################################################################\n",
        "################################################################################ REPORT CREATING THE TABLE ##########################################################################################\n",
        "################################################################################ REPORT CREATING THE TABLE ##########################################################################################\n",
        "################################################################################ REPORT CREATING THE TABLE ##########################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u2tVbZ_v6lgq",
      "metadata": {
        "id": "u2tVbZ_v6lgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f401c647-0102-4e9d-b47a-12ac837e7146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+\n",
            "|Method             |Parameters                                                                                                                                                                            |Accuracy          |\n",
            "+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+\n",
            "|Decision Tree (TVS)|maxDepth=7, impurity=entropy, maxBins=64, minInstancesPerNode=2, minInfoGain=0.0, minWeightFractionPerNode=0.0, maxMemoryInMB=256, cacheNodeIds=True, checkpointInterval=10           |0.6538461538461539|\n",
            "|Decision Tree (CV) |maxDepth=10, impurity=gini, maxBins=64, minInstancesPerNode=1, minInfoGain=0.0, minWeightFractionPerNode=0.0, maxMemoryInMB=256, cacheNodeIds=True, checkpointInterval=10             |0.5769230769230769|\n",
            "|Random Forest (TVS)|numTrees=20, featureSubsetStrategy=auto, maxDepth=10, impurity=entropy, maxBins=64, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=True, subsamplingRate=0.8 |0.7115384615384616|\n",
            "|Random Forest (CV) |numTrees=30, featureSubsetStrategy=auto, maxDepth=10, impurity=entropy, maxBins=64, minInstancesPerNode=2, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, subsamplingRate=1.0|0.6730769230769231|\n",
            "|MLP (TVS)          |layers=[15, 32, 40], blockSize=64, maxIter=200, stepSize=0.03, tol=0.0001                                                                                                             |0.4807692307692308|\n",
            "|MLP (CV)           |layers=[15, 32, 40], blockSize=64, maxIter=100, stepSize=0.03, tol=0.0001                                                                                                             |0.5               |\n",
            "+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# Stored model names, parameters and accuracies from direve for each training and fine tuning result\n",
        "paths = [\n",
        "    \"/content/drive/MyDrive/results/result_dt_tvs.csv/part*.csv\",\n",
        "    \"/content/drive/MyDrive/results/result_dt_cv.csv/part*.csv\",\n",
        "    \"/content/drive/MyDrive/results/result_rf_tvs.csv/part*.csv\",\n",
        "    \"/content/drive/MyDrive/results/result_row_rf_cv.csv/part*.csv\",\n",
        "    \"/content/drive/MyDrive/results/result_row_mlp_tvs.csv/part*.csv\",\n",
        "    \"/content/drive/MyDrive/results/result_row_mlp_cv.csv/part*.csv\"\n",
        "]\n",
        "\n",
        "# Read the first file\n",
        "df_final = spark.read.option(\"header\", \"true\").csv(paths[0])\n",
        "\n",
        "# Merge with other files one by one\n",
        "for path in paths[1:]:\n",
        "    df_part = spark.read.option(\"header\", \"true\").csv(path)\n",
        "    df_final = df_final.union(df_part)\n",
        "\n",
        "df_final.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this notebook, I applied extensive and diverse hyperparameter tuning for decision tree, random forest and multilayer perceptron classifiers\n",
        "# to each classifier using both TrainValidationSplit and CrossValidator.\n",
        "# To ensure reproducibility, I used fixed random seeds throughout training and splitting.\n",
        "# Due to the large parameter grids and 5-fold cross-validation,\n",
        "# some training sessions took up to 2 hours.\n",
        "# Since it was not feasible to run all models within the same Colab runtime session,\n",
        "# I saved each model's best result to Google Drive and later combined them into a single summary table.\n",
        "\n",
        "# The models can be reached from this drive link: https://drive.google.com/drive/folders/19dl89-bdIc8U5rFM4HUBx5mWSvJfnoHr?usp=sharing\n",
        "# The results can be reached from this drive link: https://drive.google.com/drive/folders/1yJnPVaDa2iLqKbcHi72Tj9q8XWj_bgKj?usp=sharing"
      ],
      "metadata": {
        "id": "tXcCLUdiNrOr"
      },
      "id": "tXcCLUdiNrOr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}