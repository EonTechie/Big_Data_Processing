# Big Data Processing with Apache Spark 🚀

This repository presents a complete collection of hands-on Big Data processing projects developed using **Apache Spark**.  
It showcases core components of scalable data pipelines and real-world data engineering workflows.

All notebooks are implemented independently by the repository owner (@EonTechie) as part of the **Big Data Processing course at Sabancı University**, following strict privacy and ethical principles.

---

## 💡 Project Scope

The work is organized into three main Spark paradigms:

### 🔹 Spark RDD Tasks
Low-level, fine-grained control over distributed data transformations:
- Geospatial analysis of capital cities
- Seismic foreshock-aftershock detection
- Lottery number pattern mining
- Time-series anomaly detection in currency data

### 🔹 Spark SQL & DataFrame Tasks
Relational querying and structured data analysis:
- Movie rating aggregation based on user-defined tags
- User-user similarity engine using collaborative filtering logic

### 🔹 Spark MLlib Tasks
Scalable machine learning workflows:
- Classification model tuning and evaluation on the UCI Leaf dataset using Random Forest, GBT, and more

---

## 🛠 Technologies Used
- Apache Spark (RDD, SQL, MLlib)
- PySpark
- Jupyter Notebook / Google Colab
- pandas, matplotlib (for local validation)
- Realistic datasets (MovieLens, UCI Leaf, earthquake logs, lottery records)

---

## 🧑‍💻 Author
> Developed and maintained by [@EonTechie](https://github.com/EonTechie)  
> Individual project aligned with Data Engineering roles and academic research

---

## 📌 Ethical Note
All datasets used are either publicly available or anonymized.  
Personal or sensitive information has been removed in accordance with privacy standards.
